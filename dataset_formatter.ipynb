{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\mintu\\anaconda3\\envs\\heaven\\lib\\site-packages (2.2.0)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: numpy<2,>=1.26.0 in c:\\users\\mintu\\anaconda3\\envs\\heaven\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\mintu\\anaconda3\\envs\\heaven\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\mintu\\anaconda3\\envs\\heaven\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\mintu\\anaconda3\\envs\\heaven\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\mintu\\anaconda3\\envs\\heaven\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mintu\\AppData\\Local\\Temp\\ipykernel_10436\\2162656668.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_ROCstories(csv_path):\n",
    "    \"\"\"\n",
    "    Read a CSV file containing ROCstories dataset with stories spread across columns, \n",
    "    and concatenate them\n",
    "\n",
    "    Parameters:\n",
    "    - csv_path: Path to the CSV file containing the stories.\n",
    "    - output_path: Path to save the formatted text file.\n",
    "    - delimiter: A string delimiter to separate stories. Defaults to \"<|endoftext|>\".\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    stories = df[['sentence1', 'sentence2', 'sentence3', 'sentence4', 'sentence5']].agg(' '.join, axis=1)\n",
    "\n",
    "    return stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_and_save(stories_array, output_path, delimiter=\"<|endoftext|>\"):\n",
    "    all_stories = (delimiter + \"\\n\").join(stories_array)\n",
    "    \n",
    "    with open(output_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(all_stories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_wrap_and_save(stories_array, output_path, delimiter=\"\"):\n",
    "    # start_token = \"<s>\"\n",
    "    start_token = \"\"\n",
    "    # end_token = \"</s>\"\n",
    "    end_token = \"\"\n",
    "\n",
    "    formatted_stories = [f\"{start_token}{story}{end_token}\" for story in stories_array]\n",
    "    all_stories = (delimiter + \"\\n\").join(formatted_stories)\n",
    "    \n",
    "    with open(output_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(all_stories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading both ROCStories spring 2016 and winter 2017 datasets and combining them\n",
    "datasets = ['ROCStories_spring2016', 'ROCStories_winter2017']\n",
    "\n",
    "all_stories = []\n",
    "for dataset_name in datasets:\n",
    "    csv_path = './datasets/raw/'+dataset_name+'.csv'\n",
    "    output_path = './datasets/formatted/'+dataset_name+'.txt'\n",
    "    read_stories = read_ROCstories(csv_path)\n",
    "    all_stories+=np.array(read_stories).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffling stories and splitting into training and testing data\n",
    "train_test_ratio = 0.8\n",
    "\n",
    "stories_array = np.array(all_stories)\n",
    "np.random.shuffle(stories_array)\n",
    "\n",
    "split_index = int(len(stories_array) * train_test_ratio)\n",
    "\n",
    "train_data = stories_array[:split_index].tolist()\n",
    "test_data = stories_array[split_index:].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "directory_path = \"./datasets/formatted/ROCStories\"\n",
    "\n",
    "if not os.path.exists(directory_path):\n",
    "    os.makedirs(directory_path)\n",
    "    print(\"Directory created:\", directory_path)\n",
    "\n",
    "# Saving training and testing dataset\n",
    "format_wrap_and_save(train_data, directory_path + \"/train_uf.txt\")\n",
    "format_wrap_and_save(test_data, directory_path + \"/test_uf.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "heaven",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
